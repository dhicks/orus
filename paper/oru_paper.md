---
title:  "Productivity and interdisciplinary impacts of Organized Research Units"
author:  |
 | Daniel J. Hicks 
 | Data Science Initiative, UC Davis
 | Cognitive and Information Sciences, UC Merced
 | <hicks.daniel.j@gmail.com>
numbersections: true
bibliography: oru_project.yaml
header-includes: 
    - \usepackage{graphics}
    - \usepackage{enumitem}
    - \usepackage{lineno}
    - \usepackage{multirow}
abstract: "Organized Research Units (ORUs) are non-departmental units utilized by US research universities to support interdisciplinary research initiatives, among other goals.  This study examined the impacts of ORUs at one large public research university, the University of California, Davis (UC Davis), using a large corpus of journal article metadata and abstracts for both faculty affiliated with UCD ORUs and a comparison set of other faculty.  Using regression analysis, I find that ORUs appeared to increase the number of coauthors of affiliated faculty, but did not appear to directly effect publication or citation counts.  Next, I frame interdisciplinarity in terms of a notion of discursive space, and use a topic model approach to situate researchers within this discursive space.  I find evidence that ORUs promoted multidisciplinarity rather than interdisciplinarity.  In the conclusion, drawing on work in philosophy of science on inter- and multidisciplinarity, I argue that multidisciplinarity is not necessarily inferior to interdisciplinarity."
---

<!-- https://www.mitpressjournals.org/journals/qss/sub -->

*[TODO: 
- final update of numbers
- read for coherence]*

\linenumbers

keywords: Organized Research Unit; research evaluation; productivity impacts; interdisciplinarity; topic models

# Introduction #

While there is no formal definition, Organized Research Units (ORUs) are non-departmental organizational units utilized by US research universities to support clusters of researchers working on related topics.  ORUs are typically organized internally — 
many researchers are university faculty; other researchers and staff are university employees — but funded externally.  Examples might include museums, observatories, research stations, some large physical science labs [@GeigerOrganizedResearchUnits1990 6]; the numerous small "centers" and "labs" containing just one or two faculty and limited external funding are usually not counted as ORUs [@StahlerCentersInstitutesResearch1994 542].  As the examples suggest, some ORUs support research by providing an institutional home for highly capital-intensive research activities such as specimen collections or the maintenance of large or complex instruments.  ORUs can also serve as focal points for recruiting external funding, either by demonstrating to funders that the university is actively engaged in a particular area of research [@GeigerOrganizedResearchUnits1990 9] or by providing, for example, dedicated support staff for the grant-writing and -administering cycle.  

However, at least since @GeigerOrganizedResearchUnits1990, research policy scholars have theorized ORUs as key sites for bridging barriers between disciplines (interdisciplinarity) and between academic and social interests (extradisciplinarity) [@GeigerOrganizedResearchUnits1990; @SommerServingTwoMasters1994; @EtzkowitzRoleResearchCentres1998; @SaLocalGlobalOrganized2011].  That is, it is thought that ORUs support research not just *materially* (with resources and support staff) but also *culturally* (creating a certain kind of research community).  

The aim of the current project is to examine the impact of ORUs at one large public research university — the University of California, Davis (UC Davis) — in terms of both traditional bibliometric notions of productivity (papers written, citations received) as well as interdisciplinarity.  In other words, have the ORUs at UC Davis promoted research productivity? And have they promoted interdisciplinarity?  

To answer these questions, I link rosters of faculty affiliated with ORUs to publication metadata retrieved from Scopus.  Importantly, I include not only faculty affiliated with ORUs, but also a comparison set of researchers who are affiliated with the same departments but are not affiliated with any ORU.  I also use regression models to control for variables such as career length and gender, and a directed acyclic graph and a sequence of models to examine the mechanisms by which ORUs might increase productivity.  

To examine disciplinarity and interdisciplinarity, I introduce the conceptual framework of "discursive space" and situate researchers in this space by applying topic modeling — a text mining technique — to a large corpus of journal articles abstracts by both ORU-affiliated faculty and comparison faculty.  

In brief, I find that the UC Davis ORUs have had productivity impacts, but likely did so by enabling researchers to work with more coauthors.  The analysis of "discursive space" suggests that the UC Davis ORUs have promoted multidisciplinarity rather than interdisciplinarity.  In the conclusion, drawing on work in philosophy of science on inter- and multidisciplinarity, I argue that multidisciplinarity is not necessarily inferior to interdisciplinarity.  

Note that, because I examine ORUs at a single institution, I do not claim that my results generalize to ORUs at any other institution.  

## Organized research units at UC Davis ##

At the time data collection began for this study (fall 2018), UC Davis had 8 Organized Research Units (ORUs), each of which describes itself as engaged in interdisciplinary research or education.  See tables \ref{tab.orus} and \ref{tab.interdisc}.  

| Abbreviation | Full name | Founded in |
|:-------------|:----------|:-----------|
| AQRC         | Air Quality Research Center                    | 2005 |
| BML/CMSI     | Bodega Marine Laboratory &                     | 1960 (BML)/2013 (CMSI) |
|              | Coastal Marine Science Institute               ||
| CNPRC        | California National Primate Research Center    | 1962 |
| CCC          | Comprehensive Cancer Center                    | 2002 |
| CHPR         | Center for Healthcare Policy & Research        | 1994 |
| ITS          | Institute for Transportation Studies           | 1991 |
| JMIE         | John Muir Institute of the Environment         | 1997 |
| PICN         | Program in International & Community Nutrition | 1987 |

Table:  UC Davis Organized Research Units [ORUs] examined in this study \label{tab.orus}


| ORU       | Interdisciplinary self-description      |
|:----------|:----------------------------------------|
| AQRC      | "The AQRC provides support for teams of collaborative researchers to conduct scientific, engineering, health, social and economic research that educates and informs planning and regulations for air quality and climate change." (<https://aqrc.ucdavis.edu/about>) |
| BML/CMSI  | "CMSI’s 100+ affiliated faculty and staff and over 120 graduate students and postdoctoral scholars are internationally recognized for their expertise across the full spectrum of modern marine science, including ecology, evolutionary biology, conservation biology, microbiology, coastal oceanography, environmental toxicology, geochemistry, political science, natural resource management, economics, law, corporate sustainability, and marine wildlife health." (<https://marinescience.ucdavis.edu/about>) |
| CCC       | "Center members are organized into programs, shared resources and innovation groups to transcend departmental affiliations and to be transdisciplinary, translational and transformative." (<https://health.ucdavis.edu/cancer/research/membership/index.html>) |
| CNPRC     | "The California National Primate Research Center (CNPRC) is improving health and advancing science with interdisciplinary programs in biomedical research on significant human medical conditions throughout the lifespan." (<https://cnprc.ucdavis.edu/about-us/>) |
| CHPR      | "Since our inception in 1994, the UC Davis Center for Healthcare Policy and Research (CHPR) has conducted interdisciplinary and collaborative research and research synthesis to improve health outcomes and services, educated the next generation of health services researchers, and assist policymakers in formulating effective health policies." (<http://medicine.ucdavis.edu/chpr/>) |
| ITS       | "We have a strong commitment not just to research, but interdisciplinary education and engagement with government, industry, and non-governmental organizations." (<https://its.ucdavis.edu/about/>) |
| JMIE      | "The founding of the John Muir Institute of the Environment in 1997 was a monumental achievement for visionary academics. It was the culmination of over 30 years of strong leadership in environmental research, interdisciplinary collaboration, and campus and system-wide endorsement." (<https://johnmuir.ucdavis.edu/about>) |
| PICN      | "The educational curriculum provides students with the necessary knowledge and skills to master their chosen primary disciplines, while simultaneously exposing them to interdisciplinary research methods. The training program therefore combines courses in basic biological sciences, behavioral sciences, and social sciences, as well as interdisciplinary seminars in the planning, implementation, and evaluation of nutrition programs at the community and national levels." (<https://globalnutrition.ucdavis.edu/about>) |

Table: Each ORU describes itself as engaged in interdisciplinary research or education.  All quotations were retrieved on 28 May 2020.  \label{tab.interdisc}  


Four of these ORUs are dedicated to environmental topics, broadly constructed to include ecology, conservation biology, environmental science, and environmental policy:  the Air Quality Research Center (AQRC); the Bodega Marine Lab/Coastal and Marine Science Institute (BML/CMSI); the Institute of Transportation Studies (ITS); and the John Muir Institute of the Environment (JMIE).  Of these four ORUs, JMIE is the most heterogeneous, with distinct initiatives in climate change, data science in environmental science, energy systems, polar science, and water science and policy.  

Three other ORUs are dedicated to biomedical topics:  the Comprehensive Cancer Center (CCC); the Center for Healthcare Policy and Research (CHPR); and the Program in International and Community Nutrition (PICN).  CCC has both research and clinical aspects, and as the name indicates CHPR supports both academic research and policy analysis.  PICN has a strong global focus, with active research projects in Laos; Haiti; Cameroon and Ethiopia; Burkina Faso, Ghana, and Malawi; the Gambia; Niger; Bangladesh; India; and Kenya.  

The eighth ORU, CNPRC, is organized into four "units," devoted to infectious diseases, neuroscience and behavioral research, reproductive science and regenerative medicine, and respiratory diseases.  As these labels suggest, CNPRC supports a mix of behavioral and biomedical research.  

As table \ref{tab.orus} indicates, the age of these ORUs vary substantially.  BML is the oldest of the current ORUs, founded in 1960.  CMSI, the youngest of the current ORUs, was formed in 2013 to coordinate research activities between BML (a single laboratory on the Pacific Ocean north of San Francisco) and other water research (such as the Tahoe Environmental Research Center in Incline Village, Nevada, on the north shore of Lake Tahoe in the Sierra Nevada mountains).  BML/CMSI are treated as a single unit for the purposes of this study.  

This study only considered ORU affiliations as of fall 2018; researchers who might have been affiliated with an ORU previously, but were no longer affiliated as of fall 2018, and were still actively publishing with a UC Davis affiliation during the period 2015-2017, would be considered not affiliated with any ORU.  

## Productivity and discursive impacts ##

### Productivity impacts ###

Research evaluation often focuses on measures such as publication counts, citation counts, and perhaps patents or other indicators of economic impact [@HicksBibliometricsToolResearch2012].  In the context of evaluating the effects of a particular (set of) programmatic interventions — namely, recruiting faculty to an ORU — I refer to these familiar kinds of outputs as *productivity impacts* of the intervention.  Research evaluation might also consider productivity inputs, such as grant application success rate or quantity of external research funds received.  

In this study, I examine three productivity impacts of the UC Davis ORUs.  Publication and citation counts are familiar measures of research productivity.  The third, coauthor count, is not usually used as a primary measure of productivity.  However, it is highly plausible that increased collaboration — and so an increased number of coauthors — leads at least to increased publication.  Even if increased collaboration does not itself count as increased productivity, it is one potential mechanism by which ORUs might increase productivity.  That is, along with providing (or facilitating the provision of) research funds and other material resources, ORUs might serve an important network function, encouraging researchers to work together more than they would have otherwise.  I therefore include coauthor count as a potentially significant productivity impact.  


### Discursive impacts ### 
\label{sec.discursive}

Evaluating the productivity impacts of an interdisciplinary research program or organizational unit is, methodologically and conceptually, essentially the same as evaluating a disciplinary program or organizational unit:  the same kinds of data will be collected and analyzed in the same way.  In addition, productivity impacts abstract from the content of research.  Counting publications doesn't consider what those publications are about.  

But interdisciplinary research is typically justified in terms of distinctive pragmatic goals.  For example, an unsigned editorial in *Nature* argues that "tackl[ing] society's challenges through research requires the engagement of multiple disciplines" [@MeetChallengeInterdisciplinary2016].  Following the distinctions made by Vanevar Bush and James Conant in the early Cold War period, Geiger contrasts disciplinary research with "programmatic research" [@GeigerOrganizedResearchUnits1990, 8].  Geiger argues that the norms of disciplinary research are enforced by academic departments, making them "inherently [epistemically] conservative institutions."  By contrast, ORUs (broadly understood to include museums, observatories, and extension offices) "exist to do what departments cannot do: to operate in interdisciplinary, applied, or capital-intensive areas in response to social demands for new knowledge" [@GeigerOrganizedResearchUnits1990, 17; see also @SaInterdisciplinaryStrategiesResearch2008].  

Interdisciplinary research may also have epistemic goals.  @HuutoniemiAnalyzingInterdisciplinarityTypology2010 distinguish epistemologically oriented and instrumentally oriented interdisciplinary research (and sometimes use a third category of "mixed orientation" interdisciplinary research) [see also @BruunPromotingInterdisciplinaryResearch2005 29-30, 90-1].  In a qualitative analysis of interdisciplinary research funded by the Academy of Finland, they find that the majority of interdisciplinary funding is directed towards (purely) epistemologically oriented projects [@BruunPromotingInterdisciplinaryResearch2005 104], and that, weighted by funding, epistemologically oriented research is more likely to be deeply integrative (rather than merely multidisciplinary research) than instrumentally oriented research [@BruunPromotingInterdisciplinaryResearch2005 106].  

Even when interdisciplinary research has purely epistemic goals, we would expect these goals to be distinctive from those of disciplinary research.  "Integration of various disciplinary perspectives is expected to lead to a more profound scientific understanding or more comprehensive explanations of the phenomena under study" [@HuutoniemiAnalyzingInterdisciplinarityTypology2010 85].  Different disciplines are assumed to offer complementary perspectives on the phenomenon or subject.  Then, bringing these complementary perspectives together is expected to produce qualitatively better knowledge than each could have produced on its own.  

However, different disciplinary perspectives are not necessarily complementary.  Different disciplines — or even lines of research within a given discipline — may depend on different metaphysical, epistemological, and methodological background assumptions [@KuhnStructureScientificRevolutions1996; @CartwrightDappledWorld1999; @EigenbrodeEmployingPhilosophicalDialogue2007; 
@HolbrookWhatInterdisciplinaryCommunication2013; @LonginoStudyingHumanBehavior2013; @PotochnikIdealizationAimsScience2017 ch. 7].  These sets of background assumptions may be deeply incompatible with each other, and attempts to integrate them might be frustrating and unproductive.  In other words, in this case, interdisciplinary research might be *less*, rather than more, than the sum of its disciplinary parts.  

Insofar as interdisciplinary integration has been successful in a particular case — that is, insofar as a body of research has been interdisciplinary rather than *multidisciplinary* — a variety of theoretical perspectives predict that researchers will have to have produced a collection of material and linguistic affordances spanning the divide.  In a material mode, @StarInstitutionalEcologyTranslations1989 examine "boundary objects" that circulate across disciplinary communities, serving as both shared objects of inquiry and shared sources of evidence.  Work on "trading zones" has drawn on concepts from linguistics, such as pidgins and creoles, to analyze linguistic innovation in successful cross-disciplinary interactions [@GalisonImageLogicMaterial1997; @CollinsTradingZonesInteractional2007].  For example, @AndersenConceptualDevelopmentInterdisciplinary2012 analyzes a successful collaboration between chemists and physicists, stressing the need for "interlocking mental models" such that "the same concepts may form part of multiple lexica concerned with, for example, different aspects of a phenomenon" [@AndersenConceptualDevelopmentInterdisciplinary2012 281ff].  

Citation data are often used in quantitative studies of interdisciplinarity [@WagnerApproachesUnderstandingMeasuring2011].  Cited works are classified into disciplines, often at the journal level (eg, any paper published in *Cell* counts as a biology paper), and empirical distributions of citations across disciplines are used in various metrics of variety, balance, and/or disparity/similarity [@RafolsDiversityNetworkCoherence2010].  

While citation data can help us assess the extent to which researchers draw on work across disciplines [@YoutieBibliographicCouplingNetwork2013], these data have limited ability to tell us to what extent researchers engage in different goals, pursue different research topics, or adopt different mental models.  Accessing these features of research in large-scale quantitative studies likely requires textual data and methods from text mining and natural language processing [NLP].  For example, @HicksImpactingCapabilitiesConceptual2018 suggest that text mining methods might be useful for developing measures relating to what they call "outward-facing goals," defined as "the value of research for other [extra-academic] social practices."  They focus specifically on nouns extracted from journal article abstracts, and show how clusters of these nouns can be matched to an existing taxonomy of basic human needs and values.  @HofstraDiversityInnovationParadox2020 analyze abstracts of PhD dissertations from 1977 to 2015 to identify novel word associations, which they interpret as measures of innovation.  They combine this analysis with name-based automated gender and race attributions and author-level bibliometric data from Web of Science to examine how the relationship between innovation and career success varies across demographic groups.  However, neither of these papers examined interdisciplinary research as such.  

In this paper, I propose that interdisciplinary research, as contrasted with disciplinary and multidisciplinary research, will have distinctive linguistic traces that can be detected using text mining methods.  

Conceptually, I begin with the idea of *discursive space*, the space of research topics and conceptual schemes as they manifest in language.  Figure \ref{fig.conceptual} suggests how disciplinary and interdisciplinary researchers might be configured in this discursive space.  There are two groups of disciplinary researchers, "red" and "blue."  These researchers have simple primary colors and are clustered close together, indicating that they work on similar research topics, employ similar conceptual schemes, and more generally use similar language.  The circles representing these researchers are small, indicating that they work on a relatively small set of topics.  And the clusters are in distinct areas of discursive space, indicating that they differ substantially in their research topics and conceptual schemes.  The clusters are internally homogeneous but externally heterogeneous. 

![Conceptual model of "discursive space."  Disciplinary researchers (red and blue dots) tend to be located close to members of the same discipline but far from members of other disciplines.  Interdisciplinary researchers (purple ellipses) are located in between these disciplinary clusters. \label{fig.conceptual}](img/conceptual_model.png){width="60%"}

Figure \ref{fig.conceptual} also includes two interdisciplinary researchers.  These researchers are shades of purple and are located in the space between the red and blue clusters, indicating that they use a mix of research topics, conceptual schemes, and language more generally from the two disciplines.  The ellipses representing the interdisciplinary researchers are larger, indicating that they work on a relatively large set of topics.  The shading and position of the researchers suggests that they have home departments or disciplines:  one is a bluish purple, and is closer to the blues; the other is a redish purple, and is closer to the reds.  But these interdisciplinary researchers are closer to each other than they are to their home disciplinary clusters.   

More precisely, I suggest three hypotheses based on figure \ref{fig.conceptual} and the conceptual framework of "discursive space":  compared to their departmental peers, interdisciplinary researchers will

\begin{enumerate}[label = H\arabic*., ref = H\arabic*]
\item have greater *discursive breadth*, that is, work on a wider variety of issues or use a wider variety of methods, and so have more linguistic diversity; \label{h1}
\item be further from the *discursive central tendency* of their home departments, that is, the center of departmental clusters in discursive space; and \label{h2}
\item be closer to their interdisciplinary peers than their departmental peers.  \label{h3}
\end{enumerate}

In a context where we expect an intervention to promote interdisciplinary research — for example, recruiting a faculty member to an ORU — I refer to these three hypotheses as the expected *discursive impacts* of the intervention.  This concept of discursive impacts provides a framework for evaluating ORUs and other interdisciplinary research initiatives.  Insofar as my hypotheses are correct and the UC Davis ORUs have effectively promoted interdisciplinary research, ORU-affiliated researchers should exhibit discursive impacts.  

Bibliometricians and researchers in related areas of quantitative science studies have developed various similarity measures that might be interpreted as situating researchers (or other units of analysis, such as documents or journals) relative to each other in "space" [@BoyackMappingBackboneScience2005; @LeydesdorffIndicatorsInterdisciplinarityJournals2011; @WagnerApproachesUnderstandingMeasuring2011].  Several common metrics are based on citations [@BoyackMappingBackboneScience2005 355; @LuMeasuringAuthorResearch2012 1974].  *Bibliographic coupling* operationalizes similarity in terms of outgoing citations:  two units of analysis (documents, authors, journals) are similar insofar as they cite the same sources.  *Cocitation analysis* works in the opposite direction, operationalizing similarity in terms of incoming citations:  two units are similar insofar as they are cited by the same sources.  A third citation-based metric is *intercitation analysis*, on which two units are similar insofar as they cite each other.  [Calculations of intercitation similarity usually combine citations from $x$ to $y$ with citations from $y$ to $x$ to create a symmetric statistic; @BoyackMappingBackboneScience2005 356.]  Again, while citation data carry certain kinds of information, they do not seem to tell us much about the goals, research topics, or mental models used by researchers.  

Another common similarity metric, *coword analysis* is based on word usage: two units are similar insofar as they use the same terms.  Coword data is usually combined with cosine similarity, which automatically adjusts for differences in total word count [for example, a senior researcher is likely to have a much greater total word count than a junior researcher; @LeydesdorffIndicatorsInterdisciplinarityJournals2011 88].  

In this study I use topic models to assess similarity.  Topic models begin with document-term frequency data, as in coword analysis; for example, the term "researcher" appears in a certain document 5 times.  The models then interpolate probability distributions of topics conditional on documents and terms conditional on topics.  For example, a given document may "contain" 50% topic 1, 25% topic 2, 10% topic 3, and so on.  Each topic, in turn, has a probability distribution over terms.  For example, topic 1 might have "researcher" with probability 1%, "starfish" with probability 0.5%, "cancer" with probability 0.0001%, and so on.  More formally, topic models begin with observed conditional probability distributions over terms, $p(term_w | document_i)$, and fit two probability distributions $\beta_{w,t} = p(term_w | topic_t)$ and $\gamma_{t,i} = p(topic_t | document_i)$.  

Examining the quantitative science studies literature, I was unable to find any systematic reviews that compare topic models with other approaches to measuring similarity, such as coword or cocitation analysis.  @LuMeasuringAuthorResearch2012 found modest correlations (0.40-0.48, Kendall's $\tau$-b) among coword, cocitation, and topic model-based measures of similarity; however, they used a small sample of only "the 50 most prolific authors" in a dataset and selected the particular value of $k$ used in their topic model (see below) "because it produced the most reasonable outcome by our judgment," with no further explanation or justification.  @YanScholarlyNetworkSimilarities2012 compare similarity networks constructed using bibliographic coupling, cocitation, coword, and topic model similarities.  They find that the topic model networks are highly similar to all of the other networks (0.93-0.99, cosine similarity).  However, this finding is difficult to interpret.  Most of the other networks are pairwise quite dissimilar to each other (for example, the cocitation networks' similarities for most other network types range from 0.01 to 0.65); and @YanScholarlyNetworkSimilarities2012 dichotomize the topic network by connecting nodes if and only if their topic similarities (measured using cosine similarity) are greater than 0.8.  

In quantitative science studies, topic models are often used diachronically, to examine the ways research foci have changed over time [@RehsStructuralTopicModel2020; @HanEvolutionResearchTopics2020; @MalaterreEightJournalsEight2020].  @NicholsTopicModelApproach2014 used a (previously fitted) topic model synchronically, to examine traces of interdisciplinary research in projects funded by the US National Science Foundation.  In all of these examples, a single topic model was used for analysis, and topics were interpreted as disciplines, fields, or research areas, depending on the scope of the corpus.  For example, @NicholsTopicModelApproach2014 assigned almost all topics from the fitted 1000-topic model to NSF directorates (which roughly correspond to high-level scientific fields, e.g., biology vs. computer science vs. social and behavioral science), then calculated interdisciplinarity scores based on whether these topics indicated "internal" or "external" interdisciplinarity.  @MalaterreEightJournalsEight2020 used a corpus comprising articles from 8 journals within a single field, philosophy of science, and interpreted the 25-topic model in terms of research areas.  

As the examples in the last paragraph suggest, the topics in topic models can be interpreted as disciplines (or sub-disciplinary units of analysis such as subfields).  Because the topics are clusters of terms (or, strictly speaking, probability distributions over terms), "disciplinary topics" correspond immediately to research areas, methods, and other features that readily appear in the language used within disciplines, rather than to features of disciplines as social organizations, such as power hierarchies or patterns of funding or training.  However, it is highly plausible that these two kinds of features are related, as the social organization enforces norms that regulate research areas, methods, technical terminology, and other linguistic features.  For example, @MorganPrestigeDrivesEpistemic2018 combined academic hiring data with the occurrence of keywords in article titles to examine the effect of prestige on the spread of research areas across a discipline.  

As they are usually applied, topic models are known to have two significant researcher degrees of freedom.  First, the number of topics, $k$, is a free parameter, and there is no consensus on the appropriate way to choose a value for $k$.  As far as I am aware, the current state-of-the-art in topic model development requires fitting multiple models across a range of values for $k$, then calculating various goodness-of-fit statistics on each model, such as semantic coherence, exclusivity [@RobertsStructuralTopicModels2014 6-7], the log likehood of a holdout subset of terms (i.e., not used to fit the model), the standard deviation of the residuals [which should converge to 1 at the "true" value of $k$; @TaddyEstimationSelectionTopic2012], and the number of iterations required by the algorithm to fit the model.  However, these statistics all have known limitations.  Semantic coherence favors a small number of "large" topics; exclusivity favors a large number of "small" topics.  The log likelihood and residual methods both assume that there is a "true" correct number of topics; but we would expect different numbers of topics at different levels of conceptual granularity (for example, at a coarse level of granularity "biology" might be a single topic, while at a finer level "ecology" and "molcular biology" might be distinct topics).  This last conceptual mismatch is directly related to what Carole Lee has called the "reference class problem for credit valuation in science" [@LeeReferenceClassProblem2020].  So in general these goodness-of-fit statistics will not agree on a single "best" model to use in further analysis.  

Topic interpretation introduces a second major research degree of freedom.  Topics are usually interpreted by extracting short term lists — usually the 5 or 10 highest-probability terms from each topic — and manually assigning a label to each topic based on these term lists.  Sometimes interpretation also involves a review of the documents with the highest probabilities for each topic.  Topic labels are almost always assigned by the authors of the topic model study — who may or may not have subject matter expertise in the areas covered by the corpus — and reports often provide little or no detail about how labels were validated (for example, to what extent there were substantive disagreements between labelers about how to interpret a given topic and how such disagreements were reconciled).  

To mitigate these concerns about researcher degrees of freedom, the current project does not select a single model for analysis, and does not lean on topic interpretation.  Instead, all fitted topic models are analyzed purely quantitatively to compare and situate authors relative to each other in discursive space.  That is, treating authors as "documents" in the topic model, for authors $i$ and $j$ we can locate them relative to each other in discursive space by comparing the distributions $\gamma_{\cdot,i}$ and $\gamma_{\cdot,j}$.  Then, insofar as the UC Davis ORUs have promoted interdisciplinary research, I expect this space to have the features predicted by the three hypotheses above.  Here topic models function as a technique of dimensionality reduction, moving from the high-dimensional space of all terms to the relatively low-dimension space of topics.  Comparisons of these quantitative analyses across all fitted topic models allow us to assess the robustness of results.  


# Data and methods #

In this study, my unit of analysis is individual researchers or authors, as individuated by the Scopus Author Identifier system[^auid], except for a few analytical moments in which I compare individual researchers to organizational entities (departments or ORUs).  My unit of observation is publications — paradigmatically, journal articles — retrieved from Scopus and aggregated as either author-level totals or concatenated blocks of text (specifically, the abstracts of an author's published work, treated as a single block of text).  

[^auid]: See <https://service.elsevier.com/app/answers/detail/a_id/11212/supporthub/scopus/> for current details on this system.  

Unless otherwise noted, all data used in this project was retrieved from Scopus, using either the web interface or application programming interface (API), between November 2018 and June 2019.  Due to intellectual property restrictions the data cannot be made publicly available.  Some downstream analysis files may be provided upon request.  

All data collection and analysis was conducted in R [@RCoreTeamLanguageEnvironmentStatistical2018].  The `RCurl` package was used for API access [@CRANTeamRCurlGeneralNetwork2020]; the `spaCy` Python library was used for tokenizing, lemmatizing, and tagging abstract texts with parts of speech [@SpaCyIndustrialstrengthNatural2018]; the `spacyr` package was used as an interface between R and `spaCy`; the `stm` package was used to fit topic models [@RobertsStructuralTopicModels2014]; and numerous tools in the `tidyverse` suite were used to wrangle data [@WickhamTidyverseEasilyInstall2017].  Because work on this project was interrupted for a period of approximately 18 months, software versions were not consistent across the lifespan of this project.  

All code used in data collection and analysis is available at <https://github.com/dhicks/orus>.  


## Author identification ##

In November 2018, the UC Davis Office of Research provided me with then-current rosters for each ORU.  These rosters included faculty (tenured/tenure-track faculty), "other academics" (primarily staff scientists), and "collaborators" (other researchers, who may or may not be affiliated with UC Davis and who generally did not receive funding from the ORU).  I extracted the names and ORU affiliation for all 134 affiliated faculty.  In the remainder of this paper, I refer to these ORU-affiliated faculty interchangeably as "ORU faculty" and "ORU researchers."  

In January 2019, I conducted searches using the Scopus web interface for all papers published with UC Davis affiliations in 2016, 2017, and 2018.  These searches returned 7,460, 7,771 and 8,066 results, respectively, for a total of 23,297 publications.  The metadata retrieved for these papers included names, affiliation strings, and Scopus author IDs for each author.  Using a combination of automated and manual matching, I identified author IDs for ORU-affiliated faculty, matching 125 out of 134 affiliated faculty.  I next searched the affiliation strings (from the publication metadata) for "Department of" to identify departmental affiliations for these ORU faculty.  

To identify a comparison set of researchers, I first identified all authors in the 2016-2018 Scopus results with the same departmental affiliations; that is, an author was included in this stage if they shared at least one departmental affiliation string with an ORU researcher.  This resulted in 5,645 "candidate" authors for the comparison set.  However, many of these candidates were likely graduate students and postdoctoral researchers.  Because ORU researchers are generally tenured faculty, including students and postdoctoral researchers would confound the analysis of differences between ORU and non-ORU researchers.  

I therefore used the Scopus API (application programming interface) to retrieve author-level metadata for both the ORU faculty and the candidate comparison researchers.  Specifically, I examined the number of publications and the year of first publication.  After exploratory data analysis, I filtered both ORU faculty and comparison researchers, including them for further analysis only if they met two conditions:  15 or more total publications, and first publication after 1970.  The first condition removed 60% of the candidate comparison authors, and the second was used to exclude a small number of researchers with very early first publication years (e.g., 1955) that were plausibly due to data errors.  

Note that, in the analysis below, departmental affiliations for all authors are based on the 2016-2018 Scopus results, not entire publication careers.  

After applying these filters, a total of 2,298 researchers had been selected for analysis, including 116 ORU-affiliated researchers and 2,182 "codepartmental" comparison researchers.  Figure \ref{fig.sample} shows the number of researchers in the analysis dataset for each ORU and the comparison set,[^aqrc] and figures \ref{fig.individual_net} and \ref{fig.org_net} show the structure of organizational relationships in the data.  (Note that, because some researchers are affiliated with multiple ORUs, the ORU counts are greater than 114.) 

![Number of researchers in the analysis dataset, for each ORU and the comparison set.  Note the y-axis uses a square-root scale. \label{fig.sample}](../plots/12_sample.png)

![Organizational relationships in the data.  Nodes are either organizations (ORUs or departments at UC Davis) or individuals (ORU faculty or comparison set authors).  Edges connect an individual to an organization.  Node size indicates betweenness centrality, which can be used as a measure of importance for information flow within the network \label{fig.individual_net}](../plots/12_network.png)

![Organizational relationships in the data.  Nodes are either ORUs or departments at UC Davis.  Edges connect an ORU to a department if the two organizations share a common faculty member.  Edge width and color indicates the number of such individuals. \label{fig.org_net}](../plots/12_oru_dept_network.png)

[^aqrc]: At the time of data collection, the AQRC roser also included nine "other academics," all of whom had non-faculty titles such as "Researcher," "Research Professor," or "Operations Manager."  For completeness, AQRC is included in all analyses, except those comparing researchers within a given ORU, such as distance to members of the same ORU.  

A substantial body of work in social studies of science finds consistent evidence that women academics have lower publication rates and typically receive fewer citations per publication than men academics [@SymondsGenderDifferencesPublication2006; @vanArensbergenGenderDifferencesScientific2012; @LariviereBibliometricsGlobalGender2013; @GhiasiComplianceWomenEngineers2015; @BeaudryWhichGenderGap2016; @CameronSolvingProductivityImpact2016; @ChauvinGenderDifferencesResearch2019].  (I was unable to find any literature that reported findings for nonbinary, genderqueer, or trans gender identities.  @ChauvinGenderDifferencesResearch2019 note that they "planned to include faculty identifying as nonbinary or genderqueer in a separate group," but "were unable to identify any such faculty from publicly available data sources.")  To control for these gender effects, the online tool `genderize.io` was used to attribute gender to all authors based on their first or given name.  This tool resulted in a trinary gender attribution:  woman, man, and "missing" when the tool was not sufficiently confident in either primary gender attribution.  While extremely limited, this tool allows us to account for a known confounder given the limited time available for this project.  Figure \label{fig.gender} shows the distribution of attributed gender for ORU- and non-ORU-affiliated researchers and each ORU separately.  All together, ORUs appear to be slightly more men-dominated than the comparison group.  However, there is substantial variation across ORUs; the single AQRC-affiliated faculty member is attributed as a man, more than half of PICN-affiliated faculty are attributed as women, and most ORUs have 20-40% faculty attributed as women.  In the regression analyses below, men (attributed gender) are used as the reference level for estimated gender effects.  

![Distribution of attributed gender for ORUs and comparison researchers.  A: All ORU-affiliated researchers grouped together.  B: Gender distribution within each ORU.  Gender attributions were made using an automated online tool based only on first or given name. \label{fig.gender}](../plots/12_gender.png)


## Productivity impacts ##

To investigate the productivity impacts of ORUs, I used author-level metadata from the Scopus API.  Specifically, all-career publication counts and (incoming) citation counts are both reported in the Scopus author retrieval API, and so these data were retrieved prior to the filtering step above.  Coauthor counts (total number of unique coauthors) were calculated from the article-level metadata retrieved for the text analysis steps discussed below.  Because coauthor counts, publication counts, and citation counts all varied over multiple orders of magnitude, I used the log (base 10) value for these variables in all analyses.  

I fit regression models for each of these three dependent variables, using ORU affiliation as the primary independent variable of interest and incorporating controls for gender, first year of publication (centered at 1997, which is the rounded mean first year in the analysis dataset), and dummy variables for departmental affiliation. 

Because of the log transformation of the dependent variables, the regression model coefficients can be exponentiated and interpreted as multiplicative associations.  For example, a coefficient of .5 can be interpreted as an association with a $10^{.5} \approx 3.16$-fold or $3.16 \times 100\% - 100\% = 216\%$ increase in the dependent variable.  

To account for relationships between the three dependent variables, I use the simplified directed acyclic graph (DAG) shown in figure \ref{fig.dag}.  According to this model, the number of coauthors influences the number of publications influences the number of citations.  The number of publications thus mediates between coauthors and citations, and coauthors mediates between the independent variables and publications; I also allow that coauthors might directly influence citations.  Both ORU affiliation and all of the included control variables (first year of publication, gender, department affiliation) might directly influence all three dependent variables.  

![A simplified directed acyclic graph (DAG) used to account for relationships between the productivity dependent variables. \label{fig.dag}](../plots/ORU_DAG.png)

## Discursive impacts ##

I use topic models and related text analysis methods to examine the discursive impacts of ORUs.  

### Topic modeling ###

Specifically, I first used the Scopus API to retrieve paper-level metadata for all authors in the analysis dataset.  I aimed to collect complete author histories — metadata for every publication each author had written in their entire career.  Metadata were retrieved for 128,778 distinct papers in June 2021, of which 114,461 had abstract text.^[In review, changes to the inclusion criteria used to select comparators required me to re-retrieve these metadata.  Only papers published through 2019 were included at this stage.  One Scopus author identifier no longer existed when the metadata were re-retrieved.  This identifier corresponded to one comparison author, who was excluded from analysis.]

Abstract texts were aggregated within individual authors, treating each individual author as a single "document."  For example, suppose researcher A was an author on documents 1 and 2, and researcher B was an author on documents 2 and 3.  Researcher A, as a single "document," would be represented for text analysis as the combination of abstracts 1 and 2; while researcher B would be represented as the combination of abstracts 2 and 3.  

*[rewrite]*
Vocabulary selection began by using part-of-speech tagging to identify noun phrases in each paper abstract.  Nouns are more likely than other parts of speech to carry substantive information about research topics and methods.  Noun phrases, such as "random effects model" or "unintended pregnancy," are more specific and informative than nouns alone.  414,423 distinct noun phrases were extracted from the corpus.  I then counted occurrences for each noun phrase for each other, aggregating across all of their publications.  (In the remainder of this paper, I generally use "terms" to refer to noun phrases.)  I used these author-term counts to calculate an entropy-based statistic for each term, keeping the top 11,490 terms to achieve a 5:1 ratio between authors ("documents") and terms.  Note that stopwords were not explicitly excluded at this stage; though typical lists of English stopwords do not include many nouns.  

Specifically, consider the task of identifying an author (or, more generally, a "document"), selected uniformly at random from the analysis dataset.  For $N$ authors, the  unconditional probability of drawing any given author is $p_N = \frac{1}{N}$; this distribution has entropy 
\begin{align*}
    H(P) &= \sum_{N} - p_N \log_2 p_N\\
         &= - N \frac{1}{N} \log_2 \frac{1}{N}\\
         &= \log_2 N
\end{align*}

Now suppose we are given a term $w$ ("word") drawn from the token distribution of the unknown author.  Because the uniform distribution has maximal entropy, the conditional author distribution given the term, $p(author_j | term_w)$, has a lower entropy $H_w \leq \log_2 N$.  Let $\Delta H_w = \log_2 N - H_w$.   $\Delta H_w$ measures the information about the identity of the author gained when we are given the term $w$. (This formula derives from the Kullback-Leibler divergence from the uniform distribution $p_N$ to $p(author_j | term_w)$.)  Insofar as we are given a high-information term, we can dramatically narrow down the range of possible authors.  Terms have higher information insofar as they are specific to a smaller group of authors.  

However, typically the most high-information terms will be unique to a single author, such as typos or idiosyncratic terms.  To account for this, I also calculate the order-of-magnitude of the occurrence of a term across the entire corpus, $\log_{10} n_w$.  We then take the product $\log_{10} n_w \Delta H_w$, which I represent in the code as `ndH`, and select the top terms according to this $\log_{10} n_w \Delta H_w$ statistic.  Table \ref{tab:vocab} shows the top 50 terms selected for the analysis vocabulary.  As the term list suggests, this statistic is effective at identifying terms that are clearly distinctive (in this case, to different disciplines and research fields), meaningful, and occur frequently. 

As discussed above, topic models require setting the number of topics $k$ before fitting the model, and there is no consensus on the appropriate way to choose a value for $k$.  Exploratory analysis of the author-term distributions using principal components found that *[rewrite]* 50% of the variance could be covered by 24 principal components, 80% required 167 principal components, and 90% required more than 300 principal components.  I also speculated that small-$k$ topic models might capture coarse disciplinary distinctions, but also be less stable.  Given these considerations, I fit models with 5, 10, 15, 20, 25, 50, 75, 100, 125, and 150 topics.  I calculated 5 goodness-of-fit statistics for each of these models:  semantic coherence, exclusivity, the log likehood of a holdout subset of terms (i.e., not used to fit the model), the standard deviation of the residuals, and the number of iterations required by the algorithm to fit the model.  As expected, these statistics did not indicate a uniformly "best" topic model; though $k=50$ minimized both the number of iterations and the residuals, and had a greater coherence and approximately the same exclusivity as the larger models.  

Rather than selecting a single "best" topic model, in the analysis below I either (a) conduct and report analyses using all of the topic models, highlighting $k=50$, or (b) conduct analyses for $k = 5, 25, 50, 100$, reporting all four equally.  Approach (b) is generally used when the analysis involved a complex visualization component, to keep the number of plots manageable.  

### Analyses ###

I focused my analysis on the topic distribution $\gamma_{t,i} = p(topic_t | author_i)$ for each topic model.  Recall the three hypotheses for discursive impacts, introduced in §\ref{sec.discursive}.  For \ref{h1}, I calculated "discursive breadth" for author $i$ as the entropy of the topic distribution $H_i = H(\gamma_{\cdot, i}) = \sum_t -\gamma_{t,i} \log_2 \gamma_{t,i}$.  In information theory, entropy is understood as a measure of the "width" or "breadth" of a distribution [@McElreathStatisticalRethinkingBayesian2016 267].  @RafolsDiversityNetworkCoherence2010 examine the use of diversity concepts in studies of interdisciplinarity, and analyze them into the "attributes" or "categories" of variety, balance, and disparity/similarity [@RafolsDiversityNetworkCoherence2010 266].  They note that entropy combines variety and balance [@RafolsDiversityNetworkCoherence2010 268].  @Rosen-ZviAuthortopicModelAuthors2004 use the entropy of author-topic distributions (from an author-document-topic model) "to assess the extent to which authors tend to address a single topic in their work, or cover multiple topics" [@Rosen-ZviAuthortopicModelAuthors2004 8].  At the journal level, @LeydesdorffIndicatorsInterdisciplinarityJournals2011 consider the entropy of citation distributions as a measure of interdisciplinarity.  In a factor analysis, they find it is related to a Rao–Stirling diversity measure, and conclude that “Shannon entropy qualifies as a vector-based measure of interdisciplinarity” [@LeydesdorffIndicatorsInterdisciplinarityJournals2011 96]. 

\ref{h2} and \ref{h3} both appeal to a measure of "discursive distance."  I operationalize this distance between any pair of topic distributions $\gamma_1, \gamma_2$ as Hellinger distance [@HellingerDistance2021]:  
\begin{align*}
    h(\gamma_1, \gamma_2) &= \frac{1}{\sqrt 2} \sqrt{\sum_{t} (\sqrt{\gamma_{t,1}} - \sqrt{\gamma_{t,2}})^2}\\
    &= \sqrt{1 - \sum_t \sqrt{\gamma_{t,1} \cdot \gamma_{t,2}}}
    &= \sqrt{1 - \left<\sqrt\gamma_1, \sqrt\gamma_2\right>}
\end{align*}
Hellinger distances range from 0 to 1, where 0 indicates that two distributions are the same and 1 indicates that the two distributions have completely different support.  Hellinger distance can be understood as a scaled version of the Euclidean distance between the square root vectors $\sqrt \gamma_1, \sqrt \gamma_2$; or, because the square root vectors are all unit length, as a distance measure corresponding to the cosine similarity $\frac{\left<x, y\right>}{|x||y|}$ between the square root vectors.  Cosine similarity is widely used in bibliometrics [@MingersReviewTheoryPractice2015].  

\ref{h2} requires constructing department-level topic distributions.  The `stm` package provides functions that, given a fitted topic model and an observed term distribution for a "document," estimate a conditional topic distribution $\gamma$ for that "document" (somewhat like using a fitted regression model to predict outcomes for new observations).  One simple way to construct a department-level "document" would be to aggregate the work (take the sum of term counts) of all of the authors associated with that department.  However, for the purposes of investigating \ref{h2}, this construction would lead to various problems.  First, papers by multiple authors in the department would contribute to the department-level distribution multiple times.  Second, insofar as ORU faculty are distant from the other members of their department, their contributions to the department distribution will act as outliers, and the resulting distance measures will be biased towards the ORU faculty, leading to underestimates of the effect for 
\ref{h2}.  On the other hand, if all and only non-ORU faculty members contribute to the department-level distribution, then their work would be counted twice:  first they would be used to construct the department-level distribution, and then second we would calculate their distances from this distribution.  In this case, the distance measures will be biased towards the non-ORU faculty.  

To avoid these problems, I construct department-level distributions as follows.  I first borrow an approach from machine learning [@JamesIntroductionStatisticalLearning2013 176ff], and randomly separate non-ORU authors into two discrete subsets.  The first subset — referred to as the "training" set in machine learning — is used to construct the department-level topic distributions.  The second subset — the "testing" subset — is used to make the distance comparisons, using Hellinger distance.  50% of non-ORU authors by departmental affiliation were allocated to the training set, selected uniformly at random, and the remaining non-ORU authors were assigned to the testing set.  (This means that a non-ORU author affiliated with multiple departments has the same role — testing or training — across all of their affiliations.)  Then, for each department, I aggregated all of the papers that (a) had at least one training set author and (b) did not have an ORU-affiliated author.  

After allocating authors to these subsets and constructing department-level topic distributions, I calculated "departmental distance" using Hellinger distance for all ORU-affiliated faculty and all comparison authors in the testing subset.  I used these departmental distance values as dependent variables in a series of regression models, one for each value of $k$, including first publication year, gender, log number of documents and coauthors, and department dummies as controls.  

For \ref{h3}, I made both "individual" and "organizational" comparisons for each ORU-affiliated faculty member.  At the individual level, I calculated the Hellinger distance between the ORU-affiliated researcher and other individuals, (a) in the same ORU and (b) in the same departments, and then took the minimum for both (a) and (b).  At the organizational level, I calculated the Hellinger distance between the ORU-affiliated researcher and (a) an ORU-level topic distribution, constructed by aggregating the papers authored by affiliates, and (b) the department topic distribution described above.  (Note that this means ORU-affiliated authors contribute to the ORU topic distribution, but not the department distribution.  So this construction might tend to bias distance estimates towards ORUs and away from departments.)  At both levels, (a) gives us a measure of distance within the researcher's ORU and (b) gives us a measure within the researcher's departments.  If the ORU distance is less than the departmental distance, this indicates that the ORU faculty member is closer to their ORU than to their home department, consistent with \ref{h3}.  Using both "individual" and "organizational"-level comparisons accounts for the possibility that an ORU-affiliated researcher may be quite close (in discursive space) to one or a few non-ORU-affiliated departmental colleagues but still relatively far from the "core" or "mainstream" of their department.  


# Results #

## Productivity impacts ##

Regression analyses indicate that ORU affiliation is associated with a substantial increase in the order-of-magnitude number of coauthors, 1.5-2.1-fold (1.8-fold).[^estimates]  See figure \ref{fig.reg.coauths}. ORU affiliation had a much weaker direct association with the number of publications, 1.0-1.2-fold (1.1-fold), while an order-of-magnitude increase in number of coauthors had a much stronger association, 2.9-3.3-fold (3.1-fold).  See figure \ref{fig.reg.pubs}. 

[^estimates]: Statistical estimates are reported as 95% confidence intervals followed by the maximum likelihood point estimate in parentheses.  No statistical hypothesis testing was done in this paper, and so no p-values are calculated or reported.  Confidence intervals should be interpreted as a range of values that are highly compatible with the observed data, given the modeling assumptions [@AmrheinInferentialStatisticsDescriptive2018].  A confidence interval that contains zero can still be evidence of a substantial (non-zero, say, positive) relationship insofar as the bulk of the interval is greater than zero.  

![Regression estimates for number of coauthors \label{fig.reg.coauths}](../plots/12_coauths_regression.png)

![Regression estimates for number of publications \label{fig.reg.pubs}](../plots/12_pub_regression.png)

The estimates for citations are similar.  Order-of-magnitude increases in number of publications and number of coauthors are both associated with substantial increases in the number of citations a researcher has received to date:  4.0-5.3-fold (4.6-fold) for publications, 2.4-3.0-fold (2.7-fold) for coauthors.  While controlling for these midstream dependent variables, the effect of ORU affiliation on citations received is small or perhaps even negative, .9-1.2-fold change (1.0-fold).  See figure \ref{fig.reg.cites}. 

![Regression estimates for number of citations \label{fig.reg.cites}](../plots/12_cites_regression.png)

All together, in causal terms, these regression results suggest that ORU affiliation has a substantial direct effect only on the number of coauthors.  This increase in coauthors in turn leads to increased publications and increased citations; but ORU affiliation has a much smaller direct effect on these two downstream productivity measures.  On this interpretation of the findings, ORU affiliation makes faculty more productive primarily by connecting them with collaborators.  

However, the evidence for this causal interpretation is limited, because we do not have the data to compare a researchers's number of coauthors before and after they join the ORU.  The available data are consistent with a pattern where some unobserved variable is a common cause of both ORU affiliation and coauthor count.  For example, highly gregarious and extraverted faculty members might tend to have more coauthors and also be more likely to be invited to join an ORU.  



## Discursive space ##

A list of the 5 highest-probability terms from each topic in the $k=50$ model is provided in \ref{tab:beta}.  As discussed above, I do not label these topics, and my analysis does not depend on the content of the term lists.  I include this lists so that readers can judge for themselves to what extent the topic model has identified intelligible research areas and disciplinary subfields.  

Before analyzing the discursive impacts of ORUs, I visualize "discursive space," based on the pairwise Hellinger distance between author-topic distributions.  In figure \ref{fig.mds}, these pairwise distances are represented in 2-dimensional space using the t-SNE algorithm [@vanderMaatenVisualizingDataUsing2008].  This algorithm uses an information-theoretic approach to represent high-dimensional relationships (here, the topic spaces) in two dimensions.  The algorithm is widely used in fields such as computational biology.  But it is designed to emphasize local topology rather than global geometry.  This means that a t-SNE visualization indicates nearness, but that distances in the visualization do not necessarily correspond to distances in the original high-dimensional space.  In particular, t-SNE tends to organize points into visual clusters.  Two clusters might be visually far apart but relatively close in the original high-dimensional space.  Figure \ref{fig.mds_dept} shows these same positions for the $k=50$ topic model, faceted by department, for departments with 50 or more authors in the dataset.  

Figure \ref{fig.mds} shows t-SNE visualizations across 4 values of $k$ (numbers of topics).  The visualizations suggest complex archipelagoes.  Some ORUs, such as PICN and ITS, have all of their affiliates in one or a few clusters (recall that AQRC has a single faculty affiliate in these data).  For others, such as BML/CMSI and CHPR, most of their affiliates are clustered near the ORU-level topic distribution (indicated by the square), but there are also a few further-flung affiliates.  Again, note that the t-SNE algorithm can place two clusters visually far apart even when they are close in the original high-dimensional space.  

![Visualization of "discursive space."  Panels correspond to different values of $k$ (number of topics).  Circles are authors; non-ORU-affiliated authors are indicated with translucent yellow points.  Squares and diamonds represent ORU- and department-level topic distributions, respectively.  Point positions are calculated using t-SNE on the pairwise Hellinger distance between author-topic distributions.  Ellipses indicate bounds on the researchers affiliated with each ORU (based on the convex hull). \label{fig.mds}](../plots/12_mds.png)

Figure \ref{fig.mds_dept} breaks out the visualization of "discursive space" by department for large departments (50 or more authors in the dataset).  While a few departments are tightly clustered into a single island, most are somewhat scattered, with authors distributed across the visualization.  The visualization does not suggest that ORU-affiliated authors tend to be further from the department-level "center" than their colleagues.  

![Visualization of "discursive space" for departments with 50 or more authors in the dataset, $k=50$ topic model.  Circles are authors; non-ORU-affiliated authors are indicated with translucent yellow points.  Diamonds represent the department-level topic distribution.  Positions within each panel are the same as those calculated in figure \ref{fig.mds}.\label{fig.mds_dept}}](../plots/12_mds_dept.png)

Figures \ref{fig.dept_gamma} and \ref{fig.oru_gamma} show the department- and ORU-level topic distributions.  These figures indicate that, for most departments and ORUs and except for $k=5$, work is clustered into just a few topics.  Figure \ref{fig.dept_gamma} suggests that the topic models are picking up discursive differences between disciplines.  

![Department-level topic distributions.  Panels correspond to different values of $k$.  These distributions are constructed by taking a sample of non-ORU-affiliated authors from the department, aggregating all of their papers that were not written by any ORU-affiliated author, then processing the aggregate through the fitted topic models.  The visualization indicates that, except for $k=5$, non-ORU-affiliated work in most departments occurs in just a few topics. \label{fig.dept_gamma}](../plots/12_dept_gamma.png)

![ORU-level topic distributions.  Panels correspond to different values of $k$.  These distributions are constructed by aggregating all papers by authors affiliated with the ORU, then processing the aggregate through the fitted topic models.  The visualization indicates that, except for $k=5$, work in most ORUs is clustered into 1 or 2 topics.  CNPRC and JMIE are notable exceptions, with work spread more evenly across 4-5 topics. \label{fig.oru_gamma}](../plots/12_oru_gamma.png)



## Discursive impacts ##

### Discursive breadth ###

\ref{h1} states that ORU interdisciplinarity may lead to increased "discursive breadth."  Above I proposed to measure this as the entropy of an author's topic distribution.  Figure \ref{fig.reg.entropy} shows the coefficient estimates for the association between topic entropy and ORU affiliation across all topic models, controlling for first year of publication, gender, department affiliation, and logged number of documents and coauthors.  Across all models, confidence intervals generally cover from -0.15 to 0.2 bits, with point estimates in the range -0.1 to 0.1 bits.  Using 0.5 bits ("half of a coin flip") as a threshold for substantive difference (that is, treating any value between -0.5 and 0.5 as too small to be interesting), the models uniformly indicate that the difference in discursive breadth between ORU authors and their peers is *trivial*.  \ref{h1} does not seem to be supported.  

![Coefficient estimates for association between topic entropy and ORU affiliation across values of $k$ (number of topics).  Whiskers are 95% confidence intervals.  Regression models include as controls attributed gender, first year of publication, number of publications, number of coauthors, and departmental dummies.   $k=50% is highlighted because it was judged "best" by some but not all goodness-of-fit statistics.  \label{fig.reg.entropy}](../plots/12_entropy_regression.png)  

### Departmental distance ###

\ref{h2} states that ORU interdisciplinarity may lead to increased departmental distance, that is, increased Hellinger distance from the department-level topic distribution.  Figure \ref{fig.reg.dept_dist} shows coefficient estimates for the association between departmental distance and ORU affiliation across values of $k$ (number of topics), along with 95% confidence intervals.  Here the estimates may appear to support \ref{h2}, as the estimates are generally positive.  However, most confidence intervals end well below 0.06, and (except for $k=5$) point estimates are all in the the range 0.02-0.04.  Recall that Hellinger distance is on a 0-1 scale.  On this scale, distances less than 0.05 would seem to be trivial.  That is, there does not seem to be a meaningful difference between ORU faculty and the mean of their departmental peers, and so \ref{h2} does not appear to be supported either.  

![Coefficient estimates for association between distance to departmental mean distribution and ORU affiliation across values of $k$ (number of topics).  Whiskers are 95% confidence intervals.  Regression models include as controls attributed gender, first year of publication, number of publications, number of coauthors, and departmental dummies.   $k=50% is highlighted because it was judged "best" by some but not all goodness-of-fit statistics.  \label{fig.reg.dept_dist}](../plots/12_dept_dist_reg.png)  

It might be suspected that departmental distance effects could vary across ORUs.  Figure \ref{fig.reg.dept_dist_fixed} reports coefficient estimates for ORU dummy variables, rather than the binary yes/no ORU affiliation used above; "no ORU affiliation" is used as the contrast value for the ORU dummies.  

![Coefficient estimates for association between distance to departmental mean distribution and ORU affiliation across values of $k$ (number of topics).  Whiskers are 95% confidence intervals.  Regression models include as controls attributed gender, first year of publication, number of publications, number of coauthors, and departmental dummies. "No ORU affiliation" is used as the contrast value. Dashed lines indicate the thresholds for trivial values at ±.05.  $k=50% is highlighted because it was judged "best" by some but not all goodness-of-fit statistics.    \label{fig.reg.dept_dist_fixed}](../plots/12_dept_dist_fixed_reg.png)

Figure \ref{fig.reg.dept_dist_fixed} does indeed suggest that the potential effects of ORU affiliation on departmental distance do vary across ORUs, albeit still to a limited extent.  Except for CHPR and JMIE, for at least some values of $k$ the point estimate is greater than 0.10.  There does seem to be some evidence of a non-trivial effect for CNPRC, PICN, and AQRC (though recall that this last had only a single faculty affiliate during the analysis period).  

### ORU-department relative distance ###

\ref{h3} proposes that ORU interdisciplinarity leads researchers to be closer to their ORU peers than their (non-ORU-affiliated) departmental peers in discursive space.  Figure \ref{fig.silhouette} shows scatterplots for minimal distances to both kinds of peers, for each ORU and 4 values of $k$.  In these scatterplots, the dashed line indicates $y=x$.  Points above this line are closer to ORU peers than departmental peers, and so these points would support \ref{h3}.  

![Minimal distance to ORU peers vs. departmental peers.  Both x- and y-axes are on Hellinger distance scale (0-1).  The dashed line in each panel indicates $y=x$.  Points above this line are closer to ORU peers than departmental peers, supporting \ref{h3}.  Note that comparisons to the dashed line should be made *vertically* or *horizontally*.    \label{fig.silhouette}](../plots/12_oru_dept_min_dist.png)

For most ORUs, across values of $k$, most researchers are located near or somewhat below the dashed line.  This means that researchers are typically *equidistant or closer to their closest departmental peers* than their closest ORU peers.  

Because distance comparisons in scatterplots can be misleading (comparing vertical distance to the dashed line, not Euclidean distance), figure \ref{fig.ridges} shows the distribution of these comparisons.  In this figure, positive x-axis values would indicate that departmental distance is greater than ORU distance, which would support \ref{h3}.  In this figure, modal and median values are all negative or near 0, thus conflicting.  While there are a few exceptional individuals, ORU faculty are generally *equidistant from or closer to their closest departmental peers*.  As with the other two hypotheses, these findings conflict with \ref{h3}.  

![Comparison of minimal ORU and departmental distances, across ORUs and values of $k$.  Positive x-axis values indicate that departmental distance is greater than ORU distance, supporting \ref{h3}.  Dashed vertical line indicates 0; solid lines within densities indicate median values; and small vertical dashes indicated individual values.\label{fig.ridges}](../plots/12_oru_dept_min_dist_ridges.png)

Figures \ref{fig.silhouette_org} and \ref{fig.ridges_org} show distance comparisons to the ORU- and department-level distributions.  For most ORUs and most values of $k$, the median and modal differences are positive, indicating that generally *ORU researchers are closer to the ORU than their departments*, which is consistent with \ref{h3}.  This tendency is most obvious for PICN, with a median $>0.50$ on the Hellinger scale for $k>5$.  The one exception is JMIE, which has a negative median across all values of $k$.  

![Distance to ORU- and department-level topic distributions.  Both x- and y-axes are on Hellinger distance scale (0-1).  The dashed line in each panel indicates $y=x$.  Points above this line are closer to ORU peers than departmental peers, supporting \ref{h3}.  Note that comparisons to the dashed line should be made *vertically* or *horizontally*.    \label{fig.silhouette_org}](../plots/12_oru_dept_org_dist.png)

![Comparison of distance to ORU- and department-level topic distributions, across ORUs and values of $k$.  Positive x-axis values indicate that departmental distance is greater than ORU distance, supporting \ref{h3}.  Dashed vertical line indicates 0; solid lines within densities indicate median values; and small vertical dashes indicated individual values.\label{fig.ridges_org}](../plots/12_oru_dept_org_dist_ridges.png)

All together, the distance comparisons suggest the following.  ORU faculty are often quite close to some individual non-ORU researchers in their home departments, but still relatively far from the "center" of the department as a whole.  ORU faculty are not the only members of their departments engaged in cross-disciplinary research, but they are still somewhat outside of the disciplinary mainstream.  As originally posed, \ref{h3} is ambiguous between individual-level and organizational-level comparisons.  The individual reading is not supported here, but there is support for the organizational-level reading.  


# Discussion #

The analysis of productivity impacts suggests that ORUs have increased the productivity of affiliated faculty at UC Davis.  Specifically, the sequence of regression models suggests that ORUs have increased the number of coauthors that affiliated faculty have; that this increased collaboration leads to increased publications and citations; but that ORUs have had at most a small direct effect on publications and citations.  

Due to the limitations of the study design and data we cannot be sure whether this relationship is indeed an effect of ORUs on productivity.  The data are also compatible with an effect of productivity on ORUs (that is, perhaps ORUs have tended to recruit faculty who already tend to be more productive), an unmeasured common cause (for example, perhaps more extroverted faculty have tended both to be more productive and to be recruited to ORUs), or indeed a combination of multiple causal relationships.  A more complex possibility, in line with the Matthew Effect *[cites]*, is that ORUs may have enhanced pre-existing trends, by directing more resources towards faculty who already tended to be more productive and have more coauthors.  

Bracketing concerns about causal inference and whether these results can be generalized to other universities, the productivity findings of this study suggest that ORUs — and similar infrastructure for interdisciplinary research — may have a key social network formation role.  @HicksNationalRoboticsInitiative2019 and @HicksNetworkAnalysisEvaluate2019 also analyzed specific interdisciplinary research funding programs, finding evidence that these programs appeared to be effective at supporting novel collaborations and stimulating the formation of a new research community, respectively.  

*[check interpretation]*
Turning to discursive impacts, the data do not appear to be compatible with hypotheses 1 or 2, discursive breadth or departmental distance; nor with hypothesis 3, relative distance from ORU vs. department, when read individualistically.  Here, the expected effects of interdisciplinarity (rather than multidisciplinarity) do not appear.  These results suggest that ORUs at UC Davis have fostered multidisciplinarity rather than interdisciplinarity.  

However, there is support for hypothesis 3, relatively distance from ORU vs. department, if this comparison is interpreted at an organizational rather than individual level.  Here there is some reason to think that most UC Davis ORUs have moved researchers away from the mainstream of their own disciplines and towards distinctive research questions and methods.  But, thinking of the results for hypothesis 2, non-ORU-affiliated researchers are not homogenous, and many of them move just as far from the disciplinary "center".  ORUs may have influenced the *direction* of their affiliated researchers' movement through discursive space, rather than the *magnitude*.  

JMIE is an important exception to the pattern observed in hypothesis 3.  The median JMIE researcher is closer to their home department than to the JMIE topic distribution.  JMIE also has the widest topic distribution.  Where ORUs such as PICN and BML/CMSI appear to be tightly focused on a small set of issues (represented by a single topic), JMIE appears to be much more heterogeneous (spread across several topics).  While there is some evidence that most of the ORUs have fostered interdisciplinarity, the evidence from JMIE consistently points to multidisciplinarity.  

Multidisciplinarity is often seen as inferior to interdisciplinarity; for example, Holbrook defines multidisciplinarity as "the (*mere*) juxtaposition of two or more academic disciplines focused on a single problem" [@HolbrookWhatInterdisciplinaryCommunication2013 1867, parentheses in original, my emphasis].  However, interdisciplinary research faces serious challenges that multidisciplinary research might avoid.  First, genuine integration across interdisciplinary lines is extremely difficult, not merely for pragmatic/logistical reasons but for deep conceptual reasons as well.  @EigenbrodeEmployingPhilosophicalDialogue2007 argue research disciplines are divided not only by subject matter, but also by conceptual frameworks, research methods, standards of evidence, emphasis on the social (rather than narrowly epistemic) value of research, and metaphysical assumptions about the nature of the world (56-7).  Drawing on philosophy of language and philosophy of science, they develop a workshop-style intervention designed to make these divides explicit in cross-disciplinary teams [@ORourkePhilosophicalInterventionCrossdisciplinary2012]; although clarifying the problem is not the same as solving it.  
@HolbrookWhatInterdisciplinaryCommunication2013 takes this kind of insight further, drawing on the work of Kuhn and MacIntyre to argue that interdisciplinary collaboration is conceptually incoherent because it would require crossing the boundaries of incommensurable conceptual schemes (1874).  Even if we don't follow @HolbrookWhatInterdisciplinaryCommunication2013 to this radical conclusion, we can recognize the point that interdisciplinary research will face deep communicative-conceptual challenges.  

In addition, @BristerDisciplinaryCaptureEpistemological2016 notes that cross-disciplinary collaborations can exemplify the same status and power hierarchies as academia more generally, leading to a phenomenon that she calls "disciplinary capture."  For example, as a natural science biology generally has higher status than anthropology; this hierarchy appeared in a collaboration between biologists and anthropologists, with the result that "Both groups of scientists ... perceived that conservation activities are dominated by biological research" [@BristerDisciplinaryCaptureEpistemological2016 86]. @FernandezPintoEconomicsImperialismSocial2016 makes similar points in terms of "scientific imperialism" [@FernandezPintoEconomicsImperialismSocial2016].  

Because of these challenges, interdisciplinary research may be difficult, to the point of being highly impractical, without specific interventions or institutional designs.  Even if successful interdisciplinary research can bring the epistemic and social benefits that it is supposed to, these may not be worth the costs required in particular cases.  So, all things considered, in particular cases multidisciplinarity might be preferable to interdisciplinarity.  


# Acknowledgments #

DJH's postdoctoral fellowship at UC Davis was funded by a gift to the university from Elsevier.  The funder had no influence on the design, data collection, analysis, or interpretation of this study. 

\newpage
\appendix

# Supplemental tables #

\input{../data/09_vocab.tex}

\newpage
\input{../plots/12_beta.tex}